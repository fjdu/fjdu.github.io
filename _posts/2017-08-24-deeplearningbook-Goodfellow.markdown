---
layout: post
title:  "Goodfellow et al.《深度学习》"
date:   2017-08-24 Thu 10:59:26
categories: math
---

<div hidden>
\(
\newcommand\trsps{\mathsf{T}}
\)
</div>

# [8 深度模型的训练](http://www.deeplearningbook.org/contents/optimization.html)

## 8.1 学习的优化不同于普通的优化

- 经验风险最小化
  - 直接的学习目标做不到直接优化, 只能优化一个间接目标
  - 避免过拟合
  - 某些损失函数不可导，比如 0-1 loss
- 代理损失函数和提早停止
  - 对于 0-1 loss，经常用 log-likelihood 替代
  - 替代损失函数可以比原损失函数更 robust
- Batch 和 minibatch 算法
  - 目标函数往往是对训练样本的求和
  - 把全部训练样本都考虑进来的话，计算量太大
  - 随机取出少量样本进行平均
  - 算法的收敛往往不要求提供精确的梯度
  - 许多训练样本是冗余的，本来也没有提供多少信息
  - 使用全部样本的方法叫 batch 或者 deterministic
  - 每次随机取一个样本的方法叫 stochastic 或者 online 方法
    - Online 方法一般是用来描述训练样本来自数据流的情形
  - 每次随机取一部分样本的方法叫 minibatch 或者 minibatch stochastic 方法，或者就叫 stochastic 方法
    - Stochastic gradient descent (SGD)
  - Minibatch 的大小与下述因素有关
    - 较大的 batch 给出的梯度更准确，但计算量更大
    - 如果 batch 太小，多核架构的利用率不高，所以不能太小
    - 如果一个 batch 里的所有样本被并行处理，则内存占用正比于 batch 大小
    - 有些硬件架构对于大小为 2 的指数的 batch 效率较高
    - 小的 batch 自带正则化效果
      - 小 batch 会添加噪声，起到正则化的作用
      - 泛化误差在 batch size 为 1 时最好
      - 但 batch size 太小会导致运算量加大许多，因为较大的噪声要求较小的学习率
    - 不同方法使用 minibatch 的不同信息
      - 只依赖梯度的方法比较强壮
      - 依赖 Hessian 矩阵的二阶方法需要较大的 minibatch
      - 与条件数太大的矩阵的逆相乘导致误差放大
    - 对样本要随机化，避免采样过程里的相关性
    - 多个 minibatch 同步进行: 异步并行分布式计算
    - Minibatch stochastic 方法看上去像在线方法，所以可以最小化泛化误差
      - 仅当样本不被重复使用时这种解读才成立
      - 多个 epoch 能带来其它好处，所以可以接受
      - 样本数多则只用一个 epoch

## 8.2 神经网络优化的挑战

<p>
  <ul>
    <li> 传统的机器学习做法：仔细设计目标函数和约束条件来让优化问题是凸优化</li>
    <li> 病态条件 (ill-conditioning)
      <ul>
        <li> 参见第四章</li>
        <li> 优化凸函数时也会遇到</li>
        <li> 最常见的是 Hessian 矩阵的病态</li>
        <li> 导致 SGD “卡住”</li>
        <li> 二阶项高于一阶项意味着出现了病态</li>
        <li>监控 \(g^\trsps g\) 和 \(g^\trsps H g\)
          <ul>
            <li>许多情形下，梯度的模在整个学习期间并不显著变小，而 \(g^\trsps H g\) 却增长超过一个数量级</li>
            <li>结果是，虽然梯度很大，但由于曲率更大，所以学习速度很慢</li>
          </ul>
          </li>
      </ul>
      </li>
    <li> 其它场景下处理病态条件的方法不一定适用，比如牛顿法就需要大改才能用</li>
    <li> 局部极值
      <ul>
        <li>凸优化问题等价于寻找局部极小的问题</li>
        <li>神经网络几乎一定会有多个极小值，但这不一定是个大问题</li>
        <li>模型识别问题 (model identifiability)
          <ul>
            <li>如果只要训练数据足够大，就能够唯一确定模型参数，则称为可识别</li>
            <li>包含隐变量的模型不可识别，因为把隐变量交换一下模型还是等价的
            <ul>
              <li>权重空间对称性：对于神经网络，把每层各单元的权重矢量做排列，不会影响结果。如果有 \(m\) 层，每层 \(n\) 个单元，则一共有 \(n!^m\) 种等价的排列方式。</li>
              <li>输入和输出的权重和偏置的放缩不变形：极小值对应 \((m\times n)\) 维双曲面</li>
              <li>无限维的局部极小：并不会导致问题</li>
            </ul>
            </li>
          <li>怕的是取值比全局极小大很多的局部极小<ul><li>对于小神经网络，可以构造出这种来，甚至都没有隐含层</li></ul></li>
          <li>对于实际的神经网络，不清楚是否存在许多比全局极小大许多的局部极小；许多研究者认为这不是个大问题</li>
          <li>通过梯度模来判断是否是局部极小导致了问题
            <ul>
              <li>如果梯度没有变得很小，则一定不是局部极小导致的问题</li>
              <li>如果梯度变得很小，也不一定是局部极小导致的问题</li>
              </ul></li>
          </ul>
        </li>
      </ul>
      </li>
    <li> 平台，鞍点，以及其它平坦区域
      <ul>
        <li>鞍点比局部极小常见多了: 局部极小和局部极大的交叉</li>
        <li>低维：局部极小常见；高维：鞍点常见，鞍点与局部极小的数量之比按维度 \(n\) 的指数增长，这是因为局部极小要求 Hessian 矩阵的所有本征值都是正数</li>
        <li>许多随机函数的 Hessian 矩阵的本征值为正的概率在到达低 cost 区域时变大，这意味着局部极小拥有小 cost 比拥有大 cost 的可能性较大；拥有大 cost 的临界点更有可能是鞍点；拥有极高 cost 的临界点更有可能是局部极大。</li>
      </ul>
    </li>
  </ul>
</p>
