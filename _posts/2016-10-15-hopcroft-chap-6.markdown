---
layout: post
title:  "Hopcroft: Foundations of Data Science, Chapter 6, 学习，以及 VC 维数"
date:   2016-10-15 Sat 15:34:16
categories: data science
---

<p>
\(
\newcommand\trsps{\mathsf{T}}
\newcommand\V[1]{\mathbf{#1}}
\newcommand\argmax{\mathop{\mathrm{arg\,max}}}
\newcommand\argmin{\mathop{\mathrm{arg\,min}}}
\newcommand\def[1]{定义\ {#1}\ }
\newcommand\theorem[1]{定理\ {#1}\ }
\newcommand\froben[1]{\left\lVert{#1}\right\rVert_\text{F}}
\newcommand\twonorm[1]{\left\lVert{#1}\right\rVert_\text{2}}
\newcommand\norm[1]{\left|{#1}\right|}
\)
</p>

<h2>学习</h2>
<p>
所谓“学习算法” (Learning algorithms)，指的是能够解决不同领域的问题而不需要太多领域专门知识的通用工具。学习算法的任务是对一些对象进行分类。比如，如果需要一个算法来区分不同的机动车，比如轿车、卡车，以及拖车。如果使用机动车的领域知识，可以先构造一些特征，比如轮子个数，引擎功率，门的个数，车辆的长度，等等。如果有 \(d\) 个特征，则每个对象可以表示为一个 \(d\) 维向量，称为特征向量 (译者注：注意不要与 eigenvector 搞混)，向量的每个分量给出一个特征的取值。目标是要设计一个“预测”算法，使得当给定一个向量后，能够正确预测车辆的类型。早期基于规则的方法使用领域知识来发展出一套规则，比如像这样：如果轮子有四个，则是轿车。预测通过查验规则得到。
</p>

<p>
如果采用“学习”的方法，则规则开发的过程不再是针对特定领域，而是自动的。在学习阶段，领域专门知识被用来决定特征的选取，把问题变成对特征向量的分类。近一步，领域专家被请来对一些特征向量进行分类，这些向量被称为训练样例。这些训练样例会被作为学习算法的输入。专家的作用到此为止。
</p>

<p>
学习算法把加标签的训练样本作为输入，并且发展出一套规则，当这套规则应用到训练样本时，会给出正确答案。在机动车的例子中，学习算法完全不需要知道汽车领域的知识。它只是对一些 \(d\) 维向量进行处理，然后生成把 \(d\) 维空间划分成一些区域的规则，每个区域对应了轿车、卡车等。
</p>

<p>
学习算法的任务是要让规则能正确给出训练样本的分类。当然，对于目前这个有限的任务，它可以把规则设成“对每个训练样本，使用专家提供的标签”。但是，我们这里强调奥卡姆剃刀原理，意思是算法给出的规则必须比所有训练样本组成的表格简洁。这就像发展一个科学理论来解释大量观测一样，理论必须比简单罗列所有观测要简洁。
</p>

<p>
一般的任务则不只是让规则仅对训练样本正确，而且要对未知的样本正确。直观地讲，如果分类器是使用足够多的训练样本做出，则很可能它也会适用于全体样本。我们会看到，Vapnik-Chervonenkis 维数 (VC 维数) 理论证实了这个直觉。就目前而言，我们的目标是获得一个简洁的规则集合来对训练样本正确分类。这就是“学习”。
</p>

<p>
本章中我们假定所有标签都是二值的。不难看出分成多个类的问题可以约化为二值分类问题。把车子分类成轿车与非轿车，拖车与非拖车，等等，足以给出车子的类型。所以我们假定标签只有 \(+1\) 和 \(-1\)。
</p>

<p>
在 \(d\) 维空间中，最简单的规则是对平面上一条直线的推广，也就是半空间。特征的加权和是否超过了一个阈值？这样的规则可以被想象为一个限门，特征取值作为输入，计算加权和，然后基于加权和是否超过阈值来输出是或否。也可以看看交互连接的限门网络，被称为神经网络。限门有时被称为感知器，因为对人类感知的一个模型是认为感知来自大脑里的神经网络。
</p>

<h2>线性分离器，感知器算法，边际</h2>
<p>
对半空间或者说线性分离器的学习包含了 \(d\) 维空间的 \(n\) 个有标记的样本，\(\V{a}_1, \V{a}_2, \ldots, \V{a}_n\)。任务是寻找一个 \(d\) 维向量 \(\V{w}\)，以及阈值 \(b\) 使得
\begin{equation}
\begin{split}
    \V{w}^\trsps \V{a}_i > b, \text{对所有标记为} {+1} 的 \V{a}_i,\\
    \V{w}^\trsps \V{a}_i < b, \text{对所有标记为} {-1} 的 \V{a}_i.
\end{split}
\end{equation}
满足上面不等式的 \((\V{w},b)\) 组成的一对叫做线性分离器。
</p>

<p>
上面的形式是 \(\V{w}\) 和 \(b\) 的线性规划问题，可用通常的线性规划算法求解。线性规划可在多项式时间内求解，但一个更简单的算法是感知器学习算法，并且在可行解 \(\V{w}\) 存在较多余裕或者说边际时更快，虽然在一般情况下不是多项式时间的。
</p>

<p>
为了方便，我们引入 \(\hat{\V{a}}_i = (\V{a}_i, 1)\) 以及 \(\hat{\V{w}} = (\V{w}, -b)\)。假定 \(l_i\) 是 \(\V{a}_i\) 上的标签，则前述不等式可写为
\begin{equation}
(\hat{\V{w}}^\trsps \hat{\V{a}}_i) l_i > 0 \quad 1\le i\le n
\end{equation}
由于右边是 0，我们可以让 \(\norm{\hat{\V{a}}_i} = 1\)。添加额外的坐标让维数增加 1，但现在分离器包含了原点。为了简化记号，下面我们省略符号上面的帽子。
</p>

<h2>感知器学习算法</h2>
<p>
感知器学习算法简单而优美。我们希望寻找 \(\V{w}\) 使得
\begin{equation}
(\V{w}^\trsps \V{a}_i) l_i > 0 \quad 1\le i\le n
\end{equation}
此处 \(\norm{\V{a}_i}=1\)。从 \(\V{w} = l_1\V{a}_1\) 开始，选取任何满足 \((\V{w}^\trsps \V{a}_i)l_i\le0\) 的 \(\V{a}_i\)，然后把 \(\V{w}\) 替换为 \(\V{w} + l_i\V{a}_i\)。不断重复直到对每个 \(i\) 都满足 \((\V{w}^\trsps \V{a}_i) l_i > 0\)。
</p>

<p>
这个算法背后的直觉是，每次向 \(\V{w}\) 添加 \(\V{a}_i l_i\) 都让新的 \((\V{w}^\trsps \V{a}_i) l_i\) 增加了 1。对于当前的 \(\V{a}_i\) 来说这是好事，但可能对别的 \(\V{a}_j\) 不好。下面证明这个简单的过程能快速给出一个解 \(\V{w}\)，前提是解存在且拥有好的边际。
</p>

<p>
\(\def{6.1}\)  对于上述问题的一个解 \(\V{w}\)，且对所有样本都有 \(\norm{\V{a}_i}＝1\)，边际定义为超曲面 \(\{\V{x}|\V{w}^\trsps \V{x}=0\}\) 到所有 \(\V{a}_i\) 的最小距离，即 \(\argmin_i\frac{(\V{w}^\trsps\V{a}_i)l_i}{\norm{\V{w}}}\)。
</p>

<p>
如果我们不曾约定所有 \(\norm{\V{a}_i}=1\)，则可以通过放大 \(\V{a}_i\) 来人为的提高边际。如果我们不曾在边际的定义中除以 \(\norm{\V{w}}\)，则我们也可以通过放大 \(\V{w}\) 来提高边际。有趣的事情是，算法所需的步骤数只取决于任何解所能达到的最佳边际，而与 \(n\) 和 \(d\) 无关。在实践中，感知器学习算法工作得不错。
</p>

<p>
\(\theorem{6.1}\) 假定前述问题存在解 \(\V{w}^\ast\)，对应的边际为 \(\delta > 0\)。则感知器算法可以在至多 \(\frac{1}{\delta^2}-1\) 次迭代后得到解 \(\V{w}\) 使得对所有 \(i\) 满足 \((\V{w}^\trsps \V{a}_i)l_i>0\)。
</p>

<p>
证明：令 \(\norm{\V{w}^\ast}=1\)。考虑当前的 \(\V{w}\) 与 \(\V{w}^\ast\) 的夹角的余弦，也就是 \(\frac{\V{w}^\trsps\V{w}^\ast}{\norm{\V{w}}}\)。在算法执行的每一步，分母至少增加 \(\delta\)，因为
\begin{equation}
(\V{w} + \V{a}_i l_i)^\trsps \V{w}^\ast = \V{w}^\trsps \V{w}^\ast + l_i \V{a}_i^\ast\V{w}^\ast \ge \V{w}^\trsps\V{w}^\ast + \delta.
\end{equation}
而分母至多增加 1，因为
\begin{equation}
\norm{\V{w}+\V{a}_i l_i}^2 = (\V{w} + \V{a}_il_i)^\trsps (\V{w} + \V{a}_il_i) = \norm{\V{w}}^2 + 2(\V{w}^\trsps\V{a}_i)l_i + \norm{\V{a}_i}^2l_i^2 \le \norm{\V{w}}^2 + 1.
\end{equation}
注意到这是因为这个算法只对满足 \((\V{w}^\trsps \V{a}_i)l_i\le 0\) 的 \(i\) 执行操作。
</p>

<p>
经过 \(t\) 次迭代后，\(\V{w}^\trsps\V{w}^\ast \ge (t+1)\delta\)，因为最开始 \(\V{w}^\trsps\V{w}^\ast = l_1(\V{a}_1^\trsps \V{w}^\ast) \ge \delta\) 而每一步都增加了至少 \(\delta\)。类似地，\(t\) 次迭代后 \(\norm{\V{w}}^2 \le t+1\)，因为一开始是 1，而每一步最多增加 1。于是 \(\V{w}\) 和 \(\V{w}^\ast\) 之间的夹角余弦至少是 \(\frac{(t+1)\delta}{\sqrt{t+1}}\)；另外，余弦不可能超过 1，所以
\begin{equation}
\sqrt{t+1}\delta\le 1 \quad \Rightarrow\quad t\le\frac{1}{\delta^2}-1.
\end{equation}
换句话说，这个算法必须在 \(\frac{1}{\delta^2}-1\) 步内停止，并且在停止的时候 \((\V{w}^\trsps\V{a}_i)l_i>0\) 对所有 \(i\) 成立。证毕。
</p>

<p>
关于存在边际至少为 \(\delta\) 的分离器的假设有多强？我们暂时假定 \(\V{a}_i\) 是从单位超球面上均匀取出。在第二章中我们看到，对于任何通过原点的固定超平面，单位超球体的多数质量位于这个超平面的 \(O(1/\sqrt{d})\) 距离内。所以，一个固定超平面的边际超过 \(c/\sqrt{d}\) 的概率较小。但这不意味着不存在具有较高边际的超平面。按照并集上界，我们只能说存在拥有较高边际的超平面的几率等于单个超平面具有大边际的几率乘以超平面的个数，而后者是无穷。稍后我们会看到，使用基于 VC 维数的论证，当样本是从超球面随机选出时，存在拥有大边际的超平面的几率很小。因此，关于大边际分离器的存在性对于简单的随机模型来说是不合适的。但从直觉上讲，如果需要学习的东西不是很难，比如是否某个东西是轿车，那么，当模型中存在足够多的特征时，不会有很多“近似轿车”或者“近似非轿车”与真正的轿车混淆。在这样的现实问题中，均匀密度分布不是一个合理的假定。这样，大边际分离器就很有可能存在，并且前面的定理成立。
</p>

<p>
下一个问题是，边际可以有多小。假定样本 \(\V{a}_1,\ldots,\V{a}_n\) 是些拥有 \(d\) 个坐标的矢量，并且每个坐标取值为 0 或 1，而标记规则是：
如果第一个取值为 1 的坐标序号是奇数，标记为 1；如果第一个取值为 1 的坐标序号是偶数，标记为 \(-1\)。
这个规则可以表达为
\begin{equation}
  (a_{i1},a_{i2},\ldots,a_{id}) (1,-\frac{1}{2},\frac{1}{4},-\frac{1}{8},\ldots)^\trsps > 0.
\end{equation}
对于这个例子，边际可以是指数级的小。比如，对于前 \(d/10\) 个坐标都是 0 的样本，边际是 \(O(2^{-d/10})\)。
</p>

<h2>边际的最大化</h2>
<p>
本节提供一种算法来寻找最大边际分离器。边际的定义是，对于问题 \((\V{w}^\trsps\V{a}_i)l_i>0,\ 1\le i\le n\) 的一个解 \(\V{w}\) (这里 \(\norm{\V{a}_i}=1\))，\(\delta \equiv \min_i \frac{l_i (\V{w}^\trsps\V{a}_i)}{\norm{\V{w}}}\)。由于这不是 \(\V{w}\) 的凹函数，计算起来不容易。
</p>

<p>
凸优化技术一般只适用于处理凸集上凹函数的最大化和凸函数的最小化。但是，通过修改权重矢量，可以把优化目标函数变成凹函数。注意到
\begin{equation}
    l_i \left(\frac{\V{w}^\trsps\V{a}_i}{\norm{\V{w}}\delta}\right) \ge 1
\end{equation}
对每个 \(\V{a}_i\) 成立。令 \(\V{v} = \frac{\V{w}}{\delta\norm{\V{w}}}\) 为修正后的权重矢量。边际被归一化到 1。最大化 \(\delta\) 等价于最小化 \(\norm{\V{v}}\)。于是优化问题变成
\begin{equation}
    \min \norm{\V{v}}\ \text{subject to}\ l_i(\V{v}^\trsps\V{a}_i) \ge 1, \forall i.
\end{equation}
尽管 \(\norm{\V{v}}\) 是 \(\V{v}\) 的坐标的凸函数，最好还是对 \(\norm{\V{v}}^2\) 进行优化，因为后者可微。于是我们可以把问题重新表述为:
</p>

<h2>最大边际问题：</h2>
<p>
\begin{equation}
\min\ \norm{\V{v}}^2\ \text{subject to}\ l_i(\V{v}^\trsps\V{a}_i)\ge 1.
\end{equation}
对这个凸优化问题已有许多研究，有的算法利用了这个问题的特殊结构，因此比一般的凸优化算法效率高。这里我们不讨论这些改进。这个问题的最优解 \(\V{v}\) 拥有如下性质。令 \(V\) 表示让等式 \(l_i(\V{v}^\trsps\V{a}_i)=1\) 成立的那些 \(\V{a}_i\) 张成的空间。我们断言，\(\V{v}\) 在 \(V\) 中。否则 \(\V{v}\) 有垂直于 \(V\) 的分量，把这个分量减小一个无穷小量不会影响这个等式的成立，但会减小 \(\norm{\V{v}}\) 的值，这便与最优性矛盾了。如果 \(V\) 是 \(d\) 维的，则存在 \(d\) 个独立样本使得 \(l_i(\V{v}^\trsps \V{a}_i)=1\) 成立。这 \(d\) 个方程于是拥有唯一解，而 \(\V{v}\) 必定就是那个解。这些样本被称为<b>支持向量</b>，它们唯一确定了最大边际分离器。
</p>

<h2>对多数样本给出正确分类的线性分离器</h2>
<p>
对某些线性分离器而言有可能少部分样本被错误分类。回到起初的定义，我们可以问，是否存在 \(\V{w}\) 使得至少那 \(n\) 个不等式里的 \((1-\epsilon)n\) 都被满足。可惜，这个问题是 NP 困难的，并且没有好的算法来解决。思考这件事的一个好办法是，对于每个错误归类的样本，我们承受一份损失，而我们希望最小化这个损失。但这个损失函数不是连续的，而是从 0 到 1 的突跳。但是，借助一个更好的损失函数可以解决这个问题。一个可能性是引入松弛变量 \(y_i\,\ i=1,2,\ldots,n\)，这里 \(y_i\) 衡量样本 \(\V{a}_i\) 被错误划分的程度。然后我们把松弛变量加入目标函数
\begin{equation}
\begin{split}
    \min\ \twonorm{\V{v}} + c\sum_{i=1}^n y_i \\
    \text{subject to} (\V{v}^\trsps\V{a}_i)l_i \ge 1 - y_i \\
    y_i\ge 0 i = 1,2,\ldots,n
\end{split}
\end{equation}
</p>

<p>
</p>

<p>
</p>

<p>
</p>

<p>
</p>

<p>
</p>

<p>
</p>

<p>
</p>
