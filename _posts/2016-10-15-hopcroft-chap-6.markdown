---
layout: post
title:  "Hopcroft 《数据科学基础》第六章：学习，以及 VC 维数"
date:   2016-10-15 Sat 15:34:16
categories: data science
---

<p>
\(
\newcommand\trsps{\mathsf{T}}
\newcommand\V[1]{\mathbf{#1}}
\newcommand\argmax{\mathop{\mathrm{arg\,max}}}
\newcommand\argmin{\mathop{\mathrm{arg\,min}}}
\newcommand\def[1]{定义\ {#1}\ }
\newcommand\theorem[1]{定理\ {#1}\ }
\newcommand\lemma[1]{引理\ {#1}\ }
\newcommand\froben[1]{\left\lVert{#1}\right\rVert_\text{F}}
\newcommand\normtwo[1]{\left\lVert{#1}\right\rVert^\text{2}}
\newcommand\norm[1]{\left|{#1}\right|}
\newcommand\varphI{\V{\varphi}}
\newcommand\xx{\V{x}}
\newcommand\aa{\V{a}}
\newcommand\vv{\V{v}}
\newcommand\ww{\V{w}}
\)
</p>

<h2>6.1 学习</h2>
<p>
所谓“学习算法” (Learning algorithms)，指的是能够解决不同领域的问题而不需要太多领域专门知识的通用工具。学习算法的任务是对一些对象进行分类。比如，如果需要一个算法来区分不同的机动车，比如轿车、卡车，以及拖车。如果使用机动车的领域知识，可以先构造一些特征，比如轮子个数，引擎功率，门的个数，车辆的长度，等等。如果有 \(d\) 个特征，则每个对象可以表示为一个 \(d\) 维向量，称为特征向量 (译者注：注意不要与 eigenvector 搞混)，向量的每个分量给出一个特征的取值。目标是要设计一个“预测”算法，使得当给定一个向量后，能够正确预测车辆的类型。早期基于规则的方法使用领域知识来发展出一套规则，比如像这样：如果轮子有四个，则是轿车。预测通过查验规则得到。
</p>

<p>
如果采用“学习”的方法，则规则开发的过程不再是针对特定领域，而是自动的。在学习阶段，领域专门知识被用来决定特征的选取，把问题变成对特征向量的分类。近一步，领域专家被请来对一些特征向量进行分类，这些向量被称为训练样例。这些训练样例会被作为学习算法的输入。专家的作用到此为止。
</p>

<p>
学习算法把加标签的训练样本作为输入，并且发展出一套规则，当这套规则应用到训练样本时，会给出正确答案。在机动车的例子中，学习算法完全不需要知道汽车领域的知识。它只是对一些 \(d\) 维向量进行处理，然后生成把 \(d\) 维空间划分成一些区域的规则，每个区域对应了轿车、卡车等。
</p>

<p>
学习算法的任务是要让规则能正确给出训练样本的分类。当然，对于目前这个有限的任务，它可以把规则设成“对每个训练样本，使用专家提供的标签”。但是，我们这里强调奥卡姆剃刀原理，意思是算法给出的规则必须比所有训练样本组成的表格简洁。这就像发展一个科学理论来解释大量观测一样，理论必须比简单罗列所有观测要简洁。
</p>

<p>
一般的任务则不只是让规则仅对训练样本正确，而且要对未知的样本正确。直观地讲，如果分类器是使用足够多的训练样本做出，则很可能它也会适用于全体样本。我们会看到，Vapnik-Chervonenkis 维数 (VC 维数) 理论证实了这个直觉。就目前而言，我们的目标是获得一个简洁的规则集合来对训练样本正确分类。这就是“学习”。
</p>

<p>
本章中我们假定所有标签都是二值的。不难看出分成多个类的问题可以约化为二值分类问题。把车子分类成轿车与非轿车，拖车与非拖车，等等，足以给出车子的类型。所以我们假定标签只有 \(+1\) 和 \(-1\)。
</p>

<p>
在 \(d\) 维空间中，最简单的规则是对平面上一条直线的推广，也就是半空间。特征的加权和是否超过了一个阈值？这样的规则可以被想象为一个限门，特征取值作为输入，计算加权和，然后基于加权和是否超过阈值来输出是或否。也可以看看交互连接的限门网络，被称为神经网络。限门有时被称为感知器，因为对人类感知的一个模型是认为感知来自大脑里的神经网络。
</p>

<h2>6.2 线性分离器，感知器算法，边际</h2>
<p>
对半空间或者说线性分离器的学习包含了 \(d\) 维空间的 \(n\) 个有标记的样本，\(\V{a}_1, \V{a}_2, \ldots, \V{a}_n\)。任务是寻找一个 \(d\) 维向量 \(\V{w}\)，以及阈值 \(b\) 使得
\begin{equation}
\begin{split}
    \V{w}^\trsps \V{a}_i > b, \text{对所有标记为} {+1} 的 \V{a}_i,\\
    \V{w}^\trsps \V{a}_i < b, \text{对所有标记为} {-1} 的 \V{a}_i.
\end{split}
\end{equation}
满足上面不等式的 \((\V{w},b)\) 组成的一对叫做线性分离器。
</p>

<p>
上面的形式是 \(\V{w}\) 和 \(b\) 的线性规划问题，可用通常的线性规划算法求解。线性规划可在多项式时间内求解，但一个更简单的算法是感知器学习算法，并且在可行解 \(\V{w}\) 存在较多余裕或者说边际时更快，虽然在一般情况下不是多项式时间的。
</p>

<p>
为了方便，我们引入 \(\hat{\V{a}}_i = (\V{a}_i, 1)\) 以及 \(\hat{\V{w}} = (\V{w}, -b)\)。假定 \(l_i\) 是 \(\V{a}_i\) 上的标签，则前述不等式可写为
\begin{equation}
(\hat{\V{w}}^\trsps \hat{\V{a}}_i) l_i > 0 \quad 1\le i\le n
\end{equation}
由于右边是 0，我们可以让 \(\norm{\hat{\V{a}}_i} = 1\)。添加额外的坐标让维数增加 1，但现在分离器包含了原点。为了简化记号，下面我们省略符号上面的帽子。
</p>

<h3>感知器学习算法</h3>
<p>
感知器学习算法简单而优美。我们希望寻找 \(\V{w}\) 使得
\begin{equation}
(\V{w}^\trsps \V{a}_i) l_i > 0 \quad 1\le i\le n
\end{equation}
此处 \(\norm{\V{a}_i}=1\)。从 \(\V{w} = l_1\V{a}_1\) 开始，选取任何满足 \((\V{w}^\trsps \V{a}_i)l_i\le0\) 的 \(\V{a}_i\)，然后把 \(\V{w}\) 替换为 \(\V{w} + l_i\V{a}_i\)。不断重复直到对每个 \(i\) 都满足 \((\V{w}^\trsps \V{a}_i) l_i > 0\)。
</p>

<p>
这个算法背后的直觉是，每次向 \(\V{w}\) 添加 \(\V{a}_i l_i\) 都让新的 \((\V{w}^\trsps \V{a}_i) l_i\) 增加了 1。对于当前的 \(\V{a}_i\) 来说这是好事，但可能对别的 \(\V{a}_j\) 不好。下面证明这个简单的过程能快速给出一个解 \(\V{w}\)，前提是解存在且拥有好的边际。
</p>

<p>
\(\def{6.1}\)  对于上述问题的一个解 \(\V{w}\)，且对所有样本都有 \(\norm{\V{a}_i}＝1\)，边际定义为超曲面 \(\{\V{x}|\V{w}^\trsps \V{x}=0\}\) 到所有 \(\V{a}_i\) 的最小距离，即 \(\argmin_i\frac{(\V{w}^\trsps\V{a}_i)l_i}{\norm{\V{w}}}\)。
</p>

<p>
如果我们不曾约定所有 \(\norm{\V{a}_i}=1\)，则可以通过放大 \(\V{a}_i\) 来人为的提高边际。如果我们不曾在边际的定义中除以 \(\norm{\V{w}}\)，则我们也可以通过放大 \(\V{w}\) 来提高边际。有趣的事情是，算法所需的步骤数只取决于任何解所能达到的最佳边际，而与 \(n\) 和 \(d\) 无关。在实践中，感知器学习算法工作得不错。
</p>

<p>
\(\theorem{6.1}\) 假定前述问题存在解 \(\V{w}^\ast\)，对应的边际为 \(\delta > 0\)。则感知器算法可以在至多 \(\frac{1}{\delta^2}-1\) 次迭代后得到解 \(\V{w}\) 使得对所有 \(i\) 满足 \((\V{w}^\trsps \V{a}_i)l_i>0\)。
</p>

<p>
证明：令 \(\norm{\V{w}^\ast}=1\)。考虑当前的 \(\V{w}\) 与 \(\V{w}^\ast\) 的夹角的余弦，也就是 \(\frac{\V{w}^\trsps\V{w}^\ast}{\norm{\V{w}}}\)。在算法执行的每一步，分母至少增加 \(\delta\)，因为
\begin{equation}
(\V{w} + \V{a}_i l_i)^\trsps \V{w}^\ast = \V{w}^\trsps \V{w}^\ast + l_i \V{a}_i^\ast\V{w}^\ast \ge \V{w}^\trsps\V{w}^\ast + \delta.
\end{equation}
而分母至多增加 1，因为
\begin{equation}
\norm{\V{w}+\V{a}_i l_i}^2 = (\V{w} + \V{a}_il_i)^\trsps (\V{w} + \V{a}_il_i) = \norm{\V{w}}^2 + 2(\V{w}^\trsps\V{a}_i)l_i + \norm{\V{a}_i}^2l_i^2 \le \norm{\V{w}}^2 + 1.
\end{equation}
注意到这是因为这个算法只对满足 \((\V{w}^\trsps \V{a}_i)l_i\le 0\) 的 \(i\) 执行操作。
</p>

<p>
经过 \(t\) 次迭代后，\(\V{w}^\trsps\V{w}^\ast \ge (t+1)\delta\)，因为最开始 \(\V{w}^\trsps\V{w}^\ast = l_1(\V{a}_1^\trsps \V{w}^\ast) \ge \delta\) 而每一步都增加了至少 \(\delta\)。类似地，\(t\) 次迭代后 \(\norm{\V{w}}^2 \le t+1\)，因为一开始是 1，而每一步最多增加 1。于是 \(\V{w}\) 和 \(\V{w}^\ast\) 之间的夹角余弦至少是 \(\frac{(t+1)\delta}{\sqrt{t+1}}\)；另外，余弦不可能超过 1，所以
\begin{equation}
\sqrt{t+1}\delta\le 1 \quad \Rightarrow\quad t\le\frac{1}{\delta^2}-1.
\end{equation}
换句话说，这个算法必须在 \(\frac{1}{\delta^2}-1\) 步内停止，并且在停止的时候 \((\V{w}^\trsps\V{a}_i)l_i>0\) 对所有 \(i\) 成立。证毕。
</p>

<p>
关于存在边际至少为 \(\delta\) 的分离器的假设有多强？我们暂时假定 \(\V{a}_i\) 是从单位超球面上均匀取出。在第二章中我们看到，对于任何通过原点的固定超平面，单位超球体的多数质量位于这个超平面的 \(O(1/\sqrt{d})\) 距离内。所以，一个固定超平面的边际超过 \(c/\sqrt{d}\) 的概率较小。但这不意味着不存在具有较高边际的超平面。按照并集上界，我们只能说存在拥有较高边际的超平面的几率等于单个超平面具有大边际的几率乘以超平面的个数，而后者是无穷。稍后我们会看到，使用基于 VC 维数的论证，当样本是从超球面随机选出时，存在拥有大边际的超平面的几率很小。因此，关于大边际分离器的存在性对于简单的随机模型来说是不合适的。但从直觉上讲，如果需要学习的东西不是很难，比如是否某个东西是轿车，那么，当模型中存在足够多的特征时，不会有很多“近似轿车”或者“近似非轿车”与真正的轿车混淆。在这样的现实问题中，均匀密度分布不是一个合理的假定。这样，大边际分离器就很有可能存在，并且前面的定理成立。
</p>

<p>
下一个问题是，边际可以有多小。假定样本 \(\V{a}_1,\ldots,\V{a}_n\) 是些拥有 \(d\) 个坐标的矢量，并且每个坐标取值为 0 或 1，而标记规则是：
如果第一个取值为 1 的坐标序号是奇数，标记为 1；如果第一个取值为 1 的坐标序号是偶数，标记为 \(-1\)。
这个规则可以表达为
\begin{equation}
  (a_{i1},a_{i2},\ldots,a_{id}) (1,-\frac{1}{2},\frac{1}{4},-\frac{1}{8},\ldots)^\trsps > 0.
\end{equation}
对于这个例子，边际可以是指数级的小。比如，对于前 \(d/10\) 个坐标都是 0 的样本，边际是 \(O(2^{-d/10})\)。
</p>

<h3>边际的最大化</h3>
<p>
本节提供一种算法来寻找最大边际分离器。边际的定义是，对于问题 \((\V{w}^\trsps\V{a}_i)l_i>0,\ 1\le i\le n\) 的一个解 \(\V{w}\) (这里 \(\norm{\V{a}_i}=1\))，\(\delta \equiv \min_i \frac{l_i (\V{w}^\trsps\V{a}_i)}{\norm{\V{w}}}\)。由于这不是 \(\V{w}\) 的凹函数，计算起来不容易。
</p>

<p>
凸优化技术一般只适用于处理凸集上凹函数的最大化和凸函数的最小化。但是，通过修改权重矢量，可以把优化目标函数变成凹函数。注意到
\begin{equation}
    l_i \left(\frac{\V{w}^\trsps\V{a}_i}{\norm{\V{w}}\delta}\right) \ge 1
\end{equation}
对每个 \(\V{a}_i\) 成立。令 \(\V{v} = \frac{\V{w}}{\delta\norm{\V{w}}}\) 为修正后的权重矢量。边际被归一化到 1。最大化 \(\delta\) 等价于最小化 \(\norm{\V{v}}\)。于是优化问题变成
\begin{equation}
    \min \norm{\V{v}}\ \text{subject to}\ l_i(\V{v}^\trsps\V{a}_i) \ge 1, \forall i.
\end{equation}
尽管 \(\norm{\V{v}}\) 是 \(\V{v}\) 的坐标的凸函数，最好还是对 \(\norm{\V{v}}^2\) 进行优化，因为后者可微。于是我们可以把问题重新表述为
</p>

<h3>最大边际问题：</h3>
<p>
\begin{equation}
\min\ \norm{\V{v}}^2\ \text{subject to}\ l_i(\V{v}^\trsps\V{a}_i)\ge 1.
\end{equation}
对这个凸优化问题已有许多研究，有的算法利用了这个问题的特殊结构，因此比一般的凸优化算法效率高。这里我们不讨论这些改进。这个问题的最优解 \(\V{v}\) 拥有如下性质。令 \(V\) 表示让等式 \(l_i(\V{v}^\trsps\V{a}_i)=1\) 成立的那些 \(\V{a}_i\) 张成的空间。我们断言，\(\V{v}\) 在 \(V\) 中。否则 \(\V{v}\) 有垂直于 \(V\) 的分量，把这个分量减小一个无穷小量不会影响这个等式的成立，但会减小 \(\norm{\V{v}}\) 的值，这便与最优性矛盾了。如果 \(V\) 是 \(d\) 维的，则存在 \(d\) 个独立样本使得 \(l_i(\V{v}^\trsps \V{a}_i)=1\) 成立。这 \(d\) 个方程于是拥有唯一解，而 \(\V{v}\) 必定就是那个解。这些样本被称为<b>支持向量</b>，它们唯一确定了最大边际分离器。
</p>

<h3>对多数样本给出正确分类的线性分离器</h3>
<p>
对某些线性分离器而言有可能少部分样本被错误分类。回到起初的定义，我们可以问，是否存在 \(\V{w}\) 使得至少那 \(n\) 个不等式里的 \((1-\epsilon)n\) 都被满足。可惜，这个问题是 NP 困难的，并且没有好的算法来解决。思考这件事的一个好办法是，对于每个错误归类的样本，我们承受一份损失，而我们希望最小化这个损失。但这个损失函数不是连续的，而是从 0 到 1 的突跳。但是，借助一个更好的损失函数可以解决这个问题。一个可能性是引入松弛变量 \(y_i\,\ i=1,2,\ldots,n\)，这里 \(y_i\) 衡量样本 \(\V{a}_i\) 被错误划分的程度。然后我们把松弛变量加入目标函数
\begin{equation}
\begin{split}
    &\min\ \normtwo{\V{v}} + c\sum_{i=1}^n y_i \\
    &\text{subject to}\ 
    \left.
    \begin{array}{l}
    (\V{v}^\trsps\V{a}_i)l_i \ge 1 - y_i \\
    y_i\ge 0
    \end{array}
    \right\} i = 1,2,\ldots,n
\end{split}
\end{equation}
如何设置 \(y_i\)? 可以采用 \(y_i = \max(0, 1-l_i(\V{v}^\trsps\V{a}_i))\)，这样 \(y_i\) 相当于对不等式违背的程度。因此，目标函数试图最小化总的违背程度以及边际的倒数的组合。容易看出，这等价于在相同的约束下最小化
\begin{equation}
\normtwo{\V{v}} + c\sum_{i} \left(1-l_i(\V{v}^\trsps\V{a}_i)\right)^+,
\end{equation}
其中第二项是违背程度之和。
</p>

<h2>6.3 非线性分离器，支持向量机，以及核</h2>
<p>
有些问题不存在线性分离器，但存在非线性分离器。比如，有可能存在一个多项式 \(p(\cdot)\) 使得 \(p(\V{a}_i)>1\) 对所有标记为 \(+1\) 的样本成立，而 \(p(\V{a}_i)<1\) 对所有标记为 \(-1\) 的样本成立。一个简单的例子是，考虑一个以原点为中心的单位正方形，被分为四份，其中右上和左下被标记为 \(+1\)，而右下和左上被标记为 \(-1\)。对此，\(x_1x_2>0\) 对所有 \(+1\) 样本成立，而 \(x_1x_2<0\) 对所有 \(-1\) 样本成立。于是，多项式 \(p(\cdot) = x_1x_2\) 分离了这些区域。更复杂的例子是棋盘那样交替 \(+1\) 和 \(-1\) 的模式。
</p>

<p>
若我们知道存在次数至多为 \(D\) 的多项式 \(p\) 使得一个样本 \(\V{a}\) 当且仅当 \(p(\V{a})>0\) 时被标记为 \(+1\)，则问题变成，如何寻找这样的多项式。注意到，任何 \(d\) 元整数组 \((i_1,i_2,\ldots,i_d)\) 只要满足 \(i_1+i_2+\cdots+i_d\le D\) 都决定了一个单项式 \(x_1^{i_1}x_2^{i_2}\cdots x_d^{i_d}\)。所以，多项式 \(p\) 中的单项式个数至多为把 \(d-1\) 个分隔物放入 \(D+d-1\) 个位置中的方法数，而答案是 \(\binom{D+d-1}{d-1}\le (D+d-1)^{d-1}\) (译者注：由于 \(\le\) 的存在，貌似正确答案应该是 \(\sum_{k=0}^D\binom{k+d-1}{d-1}\))。令 \(m=(D+d-1)^{d-1}\) 表示多项式个数的上限。
</p>

<p>
通过把单项式的系数看成未知数，我们可以构造一个 \(m\) 个变量的线性规划问题，对应的解给出要求的单项式。事实上，假定多项式为
\begin{equation}
    p(x_1,x_2,\ldots,x_d) = \sum_{\substack{i_1,i_2,\ldots,i_d\\ i_1+i_2+\cdots+i_d\le D}} w_{i_1,i_2,\ldots,i_d}x_1^{i_1}x_2^{i_2}\cdots x_d^{i_d},
\end{equation}
则不等式 \(p(\V{a}_i)>0\) 只是一个关于 \(w_{i_1,i_2,\ldots,i_d}\) 的线性不等式。但是，指数量级的变量个数使得即便在 \(D\) 不大的情形都失去实用价值。无论如何，这个方法理论上有些用处，它相当于把 \(d\) 维空间的样本点嵌入到 \(m\) 维空间了，使得对每个其和至多为 \(D\) 的 \((i_1,i_2,\ldots,i_d)\) 组合，都对应一个坐标，除了 \((0,0,\ldots,0)\) 之外；相应的坐标是 \(x_1^{i_1}x_2^{i_2}\cdot x_d^{i_d}\)。把这个嵌入称为 \(\varphI(\xx)\)。当 \(d=D=2\)，\(\varphI(\xx) = (x_1, x_2, x_1^2, x_2^2, x_1x_2)\)。我们需要找到 \(m\) 维矢量 \(\ww\) 使得 \(\ww\) 与 \(\varphI(\aa_i)\) 的内积当标签为 \(+1\) 时为正，标签为 \(-1\) 时为负。注意到，\(\ww\) 未必可以表达为 \(d\) 维空间某向量的 \(\varphI\) 嵌入。
</p>

<p>
与其寻找一个普通的 \(\ww\)，我们要寻找最大化边际的 \(\ww\)。相应的规划问题可以写为
\begin{equation}
    \min\ \normtwo{\ww}\ \text{subject to}\  \left(\ww^\trsps\varphI(\aa_i)\right)l_i \ge 1,\ \text{for all}\ i.
\end{equation}
主要的问题是，我们能否避免显式计算嵌入 \(\varphI\) 和矢量 \(\ww\)。实际上我们只需要隐式使用它们。这是基于一个简单但关键的观察，那就是，上述凸规划问题的任何最优解 \(\ww\) 都是 \(\varphI(\aa_i)\) 的线性组合。如果 \(\ww = \sum_i y_i\varphI(\aa_i)\)，那么 \(\ww^\trsps \varphI(\aa_j)\) 可以在不知道 \(\varphI(\aa_i)\) 而只知道 \(\varphI(\aa_i)^\trsps\varphI(\aa_j)\) 的情况下计算出。
</p>

<p>
\(\lemma{6.2}\) 上述凸规划的任意最优解 \(\ww\) 都是 \(\varphI(\aa_i)\) 的线性组合。
</p>

<p>
证明：否则的话，\(\ww\) 有垂直于所有 \(\varphI(\aa_i)\) 的分量。去掉这个分量不会影响不等式的成立，但会减小 \(\normtwo{\ww}\)，与最优性矛盾。证毕。
</p>

<p>
现在，令 \(\ww = \sum_i y_i \varphI(\aa_i)\)，这里 \(y_i\) 都是实数。注意到
\begin{equation}
    \normtwo{\ww} = \sum_{i,j}y_iy_j\varphI(\aa_i)^\trsps\varphI(\aa_j),
\end{equation}
于是凸规划可以重新表述为
\begin{equation}
\begin{split}
    &\min\ \sum_{i,j}y_iy_j\varphI(\aa_i)^\trsps\varphI(\aa_j), \\
    &\text{subject to}\ l_i\left(\sum_j y_j\varphI(\aa_j)^\trsps\varphI(\aa_i)\right) \ge 1\ \forall i.
\end{split}
\end{equation}
重要的是注意到 \(\varphI\) 本身是不需要的，只需要 \(\varphI(\aa_i)\) 之间的内积。核矩阵 \(K\) 定义为
\begin{equation}
    k_{ij} = \varphI(\aa_i)^\trsps\varphI(\aa_j).
\end{equation}
于是凸规划问题可以写为
\begin{equation}
    \min\ \sum_{i,j} y_iy_j k_{ij}\ \text{subject to}\ l_i\sum_j k_{ij}y_j \ge 1.
\end{equation}
这个凸规划被称为<b>支持向量机</b> (support vector machine; SVM)，尽管它并不是什么机器。\(K\) 的优势在于，它只有 \(n^2\) 个数，而不是 \(O(d^D)\) 个。我们不需要通过 \(\aa_i\) 计算 \(\varphI(\aa_i)\)，而只需要知道如何从 \(\aa_i\) 得到 \(K\)。相应的计算通常用封闭形式给出。比如，“高斯核”由下面式子给出
\begin{equation}
    k_{ij} = e^{-c\norm{\aa_i-\aa_j}}.
\end{equation}
稍后我们证明这的确是个核函数。
</p>

<p>
首先面临的问题是，给定一个矩阵 \(K\)，比如上面那个高斯核，我们怎么知道它是来自通过嵌入 \(\varphI\) 的两两内积得到的？这通过下面的引理给出。
</p>

<p>
\(\lemma{6.3}\) \(K\) 是一个核矩阵当且仅当它是个半正定矩阵。
</p>

<p>
证明：若 \(K\) 是半正定矩阵，则可表示为 \(K = B B^\trsps\)。定义 \(\varphI(\aa_i)\) 为 \(B\) 的第 \(i\) 行，则 \(k_{ij} = \varphI(\aa_i)^\trsps\varphI(\aa_j)\)。反过来也是明显的。证毕。
</p>

<p>
注意到，函数 \(\sum_{i,j} y_iy_j k_{ij} = y^\trsps K y\) 当且仅当 \(K\) 为半正定时是凸函数。于是支持向量机问题是凸规划问题。可以使用任何半正定矩阵作为核矩阵。
</p>

<p>
这里给出一个核矩阵的重要例子。定义 \(k_{ij} = (\aa_i^\trsps\aa_j)^p\)，这里 \(p\) 是个正整数。通过直接计算可知 \(K\) 是正定矩阵。
</p>

<p>
由此得知，对于矢量集合 \(\aa_1, \aa_2,\ldots,\aa_n\)，以及任何非负常数 \(c_1,c_2,\ldots\)，拥有绝对收敛级数展开的矩阵 \(k_{ij} = \sum_{p=0}^\infty c_p\left(\aa_i^\trsps\aa_j\right)^p\) 是正定矩阵。
</p>

<p>
\(\lemma{6.4}\) 对于矢量集合 \(\aa_1, \aa_2,\ldots,\aa_n\)，由 \(k_{ij} = e^{-\normtwo{\aa_i-\aa_j}/(2\sigma^2)}\) 给出的矩阵对于任何 \(\sigma\) 是半正定的。
</p>

<p>
证明：\(e^{-\normtwo{\aa_i-\aa_j}/(2\sigma^2)} = e^{-\normtwo{\aa_i}/2\sigma^2}e^{-\normtwo{\aa_j}/2\sigma^2} e^{\aa_i^\trsps\aa_j/\sigma^2} = e^{-\normtwo{\aa_i}/2\sigma^2}e^{-\normtwo{\aa_j}/2\sigma^2} \sum_{t=0}^\infty \left(\frac{(\aa_i^\trsps\aa_j)^t}{t! \sigma^{2t}}\right).\)  矩阵 \(l_{ij} = \sum_{t=0}^\infty \left(\frac{(\aa_i^\trsps\aa_j)^t}{t! \sigma^{2t}}\right)\) 是半正定的，所以 \(K\) 也是。
</p>

<p>
<b>例子：(高斯核的应用)</b> 考虑下述情形：样本位于平面上两条互相缠绕的曲线上，一条对应的标签是 \(+1\)，另一条对应 \(-1\)。假设样本在每条曲线上的间隔为 \(\delta\)，而两条曲线之间的最小距离是 \(\Delta \gg \delta\)。很明显，不存在一条直线能区分这两类。由于两条曲线互相缠绕，直觉上任何能区分两者的多项式必定次数很高。考虑高斯核。对于同一条曲线上的点，相邻点对应 \(k_{ij}\approx 1\)，而对其它点 \(k_{ij}\approx 0\)。对样本点重新排序，第一条曲线的在前面，第二条的在后面，则 \(K\) 具有块对角形式
\[
K = \left(\begin{array}{cc} K_1 & 0 \\ 0 & K_2\end{array}\right),
\]
并且对角元素为 1，然后随着与对角线的距离指数衰减为 0。
</p>

<p>
这个 SVM 实质上具有下述形式
\begin{equation}
\begin{split}
    &\min\ \V{y}_1^\trsps K_1 \V{y}_1 + \V{y}_2^\trsps K_2 \V{y}_2\\
    &\text{subject to}\ K_1\V{y}_1 \ge 1\ \text{and}\ K_2\V{y}_2 \le -1.
\end{split}
\end{equation}
这样就分成了两个规划问题，一个针对 \(\V{y}_1\)，一个针对 \(\V{y}_2\)。从 \(K_1=K_2\) 可知，\(\V{y}_2 = \V{y}_2\)。进一步，由于每条曲线的结构除了尾端之外基本没什么变化，\(\V{y}_1\) 的各分量的取值会近乎相等，而\(\V{y}_2\) 也一样。所以，\(\V{y}_1\) 的分量全是 \(1\)，而 \(\V{y}_2\) 的分量全是 \(-1\)。令 \(l_i=\pm1\) 为标签，则 \(y_i\) 的值提供了一个很好的简单分类器，也就是 \(l_iy_i>1\)。
</p>

<h2>6.4 强弱学习 － Boosting</h2>
<p>
</p>

<p>
</p>

<p>
</p>

<p>
</p>

<p>
</p>
