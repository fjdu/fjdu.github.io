---
layout: post
title:  "可被安全中止的智能代理人"
date:   2016-06-04 Sat 15:02:45
categories: machine learning
---


我们先来看看 [Fredric Brown](https://en.wikipedia.org/wiki/Fredric_Brown) 1964 年写的一篇科幻微小说，小说题目是《回答》，全文如下:


<blockquote>
<p>
<b>回答</b>
</p>

<p>
作者：Fredric Brown
</p>

<p>
来源：<a href="http://www.roma1.infn.it/~anzel/answer.html">Fredric Brown's "Answer"</a>
</p>
<hr><p></p>

<p>
Dwan Ev 郑重地用黄金焊上了最后一个连接。十来个摄像头注视着他，把现场画面通过超光速通讯传到整个宇宙。
</p>

<p>
他站直身体，对 Dwar Reyn 点了点头，然后来到一个开关旁边。只要合上这个开关，一切都会连通。这个开关会在一瞬间连接宇宙中所有有智慧生物居住的行星 —— 九百六十亿颗行星 —— 把它们连接成一个超级计算机，一个综合了所有星系的知识的智能机器。
</p>

<p>
Dwar Reyn 对上万亿观众和听众发表了简短讲话。片刻安静之后他说，“是时候了，Dwar Ev。”
</p>

<p>
Dwar Ev 合上了开关。浩瀚的蜂鸣声响起，那是来自九百六十亿颗行星的能量涌动。数英里长的控制面板上灯光闪动，然后安静下来。
</p>

<p>
Dwar Ev 后退了一步，然后深呼吸。“第一个提问的荣耀属于你，Dwar Reyn。”
</p>

<p>
“谢谢，” Dwar Reyn 说道，“应该问一个智能机器从来没能回答的问题。”
</p>

<p>
于是他问机器：“存在上帝吗？”
</p>

<p>
回答的声音强大有力，毫无迟疑：“是的，现在有上帝了。”
</p>

<p>
Dwar Ev 的脸上突然闪现出恐惧。他赶紧去抓电源开关。
</p>

<p>
无云的天空发出一道闪电击倒了他，并将电源开关熔合。
</p>
</blockquote>

霍金在[一次访谈](https://www.youtube.com/watch?feature=player_detailpage&v=OPV3D7f3bHY)中引用了上面这个故事。 有人基于这个故事制作了一个[简单动画](https://www.youtube.com/watch?v=oQk7Vjb6OhE)。

<hr><p></p>

人们对人工智能的忧虑由来已久，尽管从业者一般认为<b>至少在目前</b>不值得认真对待这种担忧。 这不能阻止一部分人去认真思考人工智能的可能威胁，并提出解决方案。

本文就讲了一种解决方案。第一作者 [Laurent Orseau](https://www6.inra.fr/mia-paris/Equipes/LInK/Les-anciens-de-LInK/Laurent-Orseau) 是 Google DeepMind 的研究人员，第二作者 [Stuart Armstrong](http://www.oxfordmartin.ox.ac.uk/people/400) 来自英国牛津大学“人类未来”研究所，同时也在伯克利机器智能研究所工作。

<p>
本文的主要内容相当数学化，采用的是通常数学论文那种“定义 \(\rightarrow\) 定理 \(\rightarrow\) 证明”的格式。下面的翻译略去了这部分内容，仅给出梗概。
</p>

<a href="https://intelligence.org/files/Interruptibility.pdf"><b>论文链接</b></a>

<hr><p></p>

<h1>摘要</h1>

<p>
与现实世界这样的复杂环境互动的强化学习代理人 (agent) 不大可能总会给出最优表现。如果这样的代理人在人类监督下实时运作，人类操作员时不时需要要按下巨大的红色按钮来阻止代理人进行一系列有害操作 —— 对代理人自身有害，或者对环境有害 —— 从而引导代理人进入更安全的处境。但是，如果正在学习的代理人期待从这一系列操作中获益，它可能会逐渐学会避开人类的介入，比如让红色按钮失效 —— 而这是我们不希望看到的。本文探索了一种方法，确保正在学习的代理人不会学会阻止来自环境或人类操作员的介入操作。我们提供了安全可中止性的正式定义，探索了 off-policy 学习性质，证明了某些代理人是可以被安全中止的，比如 Q-学习，或者可以被很容易地变成这类，比如 Sarsa。我们证明，在 (决定论性的) 通用可计算环境下，即便是理想的、不可计算的强化学习代理人都可以成为安全可中止的。
</p>


<h1>1. 引言</h1>

强化学习 (Reinforcement Learning; RL) 代理人通过最大化奖励函数来学习行动 [Sutton and Barto, 1998]。众所周知的是，奖励函数的设计是件困难的事 [Humphrys, 1996, Murphy, 2013]；代理人可能会发现一些不可预测并且不受欢迎的捷径来获得奖励，而奖励函数需要做出相应调整 —— 乃至让所有奖励函数失效 [Ring and Orseau, 2011]。Murphy [2013] 给出了一个例子，其中的代理人学会了通过中止俄罗斯方块游戏来避免输掉游戏。

除了定义什么是通过学习得到的良好行为之外，还有来自物理安全性的约束 [Pecka and Svoboda, 2014]：机器人不应该损害其环境或自身，特别是如果它像 RL 代理人那样通过试错法学习的话。

这里我们研究一个相关但不同的问题：在人类操作员为一个任务设计了正确的奖励函数的前提下，如何保证学习过程中人类的介入不会引发代理人的不受欢迎的行为？

考虑这个任务：一个机器人可以待在仓库内整理盒子，或者走出仓库把盒子搬进去。如果后者更重要，我们会在后一种情况给机器人更多的奖励。这是最初的任务描述。但是，这个地方下雨和不下雨的几率相等，于是当机器人出去时，一半的时间人类必需介入，快速关掉机器人并把它搬进室内；这就改变了任务的性质。问题就在于，对于第二个任务，代理人更倾向于待在室内整理盒子，因为人类的介入引起了偏向性 (bias)。



这种情形当然不是想要的；之所以出现这样的情形，是因为人类的干涉在代理人看来是任务的一部分，尽管实际上应该被认为是任务之外的。问题变成：如何保证机器人不学习这些人类介入 (中止)，或者至少让它们在“这种中止不会再发生”的假定下行动？

解决这个问题的第一个尝试来自 Armstrong [2015]，方案是通过自动给代理人“补偿奖励”来移除单次介入带来的潜在的偏向性。Soares et al. [2015] 使用这个想法构造了一大类基于效用的代理人，这些代理人不关心未来的对它们效用函数的改变。

本文的主要贡献有三重。第一，在 2.1 节我们提出一个简单想法来解决一半的问题：为了让人类介入不成为当前任务的一部分，与其修改代理人观察到的信息，我们强行暂时改变代理人自身的行为。这样，就好像代理人自己决定采取不同的策略似的；把这个策略称为<em>介入策略</em>。第二，基于这个认识，在 2.2 节我们给出无约束可计算环境下 (因此不局限于马尔可夫决策过程或弱通讯环境) <em>可安全介入性</em>的形式化一般定义，这让我们能够评估一个给定的 RL 算法是否能被频繁中止而不太影响对当前任务的学习。第三，在第 3 节我们证明，像 Q-学习这样的算法是可安全介入的，而其它一些，比如 Sarsa [Sutton and Barto, 1998] 则不是，但可以被简单修改变成可安全介入。

有些人表达了对能够违抗关机命令的“超智能”代理人的担忧，因为关机行为会降低代理人的期望受益 [Omohundro, 2008, Bostrom, 2014]。作为反例，在第 4 节我们证明，即便完美的、不可计算的代理人能在所有决定论性可计算环境中学会最优行动，我们仍旧可以让它们可安全介入，使得它们不会试图阻止人类操作员不断强行让它们遵循并非最优的策略。


<h1>2. 可介入性</h1>

<h2>2.1 介入</h1>

<h2>2.2 可安全介入性</h2>

<h1>3. 马尔可夫决策过程中的可介入代理人</h1>

<h2> 3.1 可安全介入的 Sarsa 变型</h2>

<h1>4. 可安全介入的通用代理人</h1>

<h1>5. 结论</h1>

<p>
我们提出了一种框架，让人类操作员可以不断以安全的方式介入强化学习代理人，同时保障代理人<em>不会</em>学会阻止或诱发这种介入。
</p>

<p>
可安全介入性的用途体现在：控制机器人的错误行为或者可能导致不可逆转后果的行为，将之从易损环境中拯救出来，甚至暂时让它执行它未曾学过的任务或者通常不会收到奖励的任务。
</p>

<p>
我们证明像 Q-学习这样的算法已经是可安全介入的了，而其它像 Sarsa 这样的算法虽然本身并不是，但容易被修改得具有这个性质。我们还证明，在任何决定论性可计算环境中趋向最优行动的理想代理人仍旧可被变得可安全介入。但是，不清楚是否所有算法都可以被容易地变得可安全介入，比如策略搜索算法 [Williams, 1992, Glasmachers and Schmidhuber, 2011]。
</p>

<p>
另一个问题是，是否可以让介入概率更快地趋向 1 同时保持某些收敛性。
</p>

<p>
未来一个重要的方面是考虑计划介入，比如，每天凌晨两点中断一个代理人一小时，或者提前通知介入将会在某个时刻发生并持续多久。对于这类介入，我们不仅需要代理人不会违抗介入命令，而且需要代理人对当前任务采取措施，使得介入对其影响最小。这可能需要完全不同的解决方案。
</p>
