---
layout: post
title:  "强人工智能的出现似乎不可避免"
date:   2016-03-11 Fri 02:38:19
categories: artificial intelligence
---

程序语言领域有个名言叫做“格林斯潘第十诫” (Greenspun's tenth rule)，说的是，任何足够复杂的 C 或 Fortran 程序都包含一个临时拼凑的、没有正式定义的、充满 bug 的、并且速度慢的 Common Lisp 的一半实现。并且还有个推论：用 Common Lisp 写的程序也不例外。

按照维基百科，这个“定律”本身有两种解读方式：第一种是，由于 Lisp 语言的灵活性和可扩展性，它已经包含了任何为了设计一个复杂系统理论上需要的全部功能，而用别的语言开发出的这些功能等价于 Lisp 里的相应方法。第二种是在讽刺那些复杂而高度可定制化的系统。与其自己设计一套这种系统，格林斯潘建议用一个广泛接收的、特性完备的语言，比如 Lisp。

这里要说的不是程序语言，而是人工智能，或者说，是“通过部分地模仿人类头脑为人类服务的软硬件设备”，所以我提供第三种解读：当一个系统足够复杂、功能足够强大，则必然会引入一些“自省”系统。所谓“自省”，也就是自我反省，比如问“我为什么存在”这种问题就是一种自省。为什么呢？因为要功能强大，这个系统必须回答“自己如何更好地存在”这样的问题，也就是利用搜集的各种信息发现自身局限性并且提高自身的能力，以及“如何更好地发现自身的局限性并提高自身的能力”。当一个系统具有较高水准的这种能力后，就算是拥有智能了。

如今说强人工智能不可能实现的，可能就跟不久前说电脑不可能在围棋项目上战胜人类的人一样。只要认可人是一台机器 --- 虽然是极端复杂的机器 --- 有什么理由认为同样作为机器的电脑不可能“模拟”人脑的功能呢？

人脑的“过度拟合” (overfitting) 一点不比机器的少见。

当然，“预言强人工智能必然出现”这种事，本身就是一种 overfitting。

人类与机器的不同在于，人类面对的外部环境更加复杂多变，而机器面对的输入总是有局限的。并且人类还面对自然选择。

如果说机器自我对局有可能只是在做低水平重复，那么为什么人类在相互对局中可以达到很高水平。我想，把整个围棋的历史作为一个整体看，那么最早的棋手肯定水平比现在差很多。同时人类在下棋这件事上的策略会与人类在别的方面的活动的影响，比如把对弈想象成两军交战等，于是交战的计谋策略可以用到对弈中；目前的机器学习还做不到这样，没办法把在一个领域内学到的策略应用到别的领域，不得不重新学习；当然，研究机器学习算法的人会知道借鉴不同领域的算法，但机器学习算法本身目前据我所知还做不到这一点。

我们可以设想，当计算机通过数百万张猫图学会了猫这个概念后，学到的不仅是猫这个概念，而且还应该学到，把猫推广到别的动物的方法，这样，之后只需要给计算机看几张狗图，就能学会狗的概念。

一部分语言学家常说人类的语言能力是一种天赋，或者说人类头脑里有一种天然生就的“语言器官”，否则难以理解为什么幼儿学语言那么快。也许可以理解为，人类的神经网络存在一种“原初结构”，这种结构可以通过轻微修改实现不同功能，比如视觉听觉语言之类；尽管在大脑内部也许存在先天分区，但也有很大的适应性。也许语言能力视觉能力听觉能力以及抽象思维能力是基于相同或类似的生理构造。

如果人类语言相对动物“语言”有多大不同，这种不同很可能是来自思维能力本身的不同，或者神经元初始构造的不同。

人工智能的进化相对人类而言有个优势：并行进化，或者说基因共享，意思是不同机器之间可以分享最优参数。

但这个优势同时也是劣势：基因的单一性可能导致掉入局部最低陷阱。当外界条件变化时可能不能很好应对。
