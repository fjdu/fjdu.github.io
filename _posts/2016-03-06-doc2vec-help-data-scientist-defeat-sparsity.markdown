---
layout: post
title:  "Doc2Vec 如何帮助数据科学家战胜稀疏性"
date:   2016-03-06 Sun 19:56:54
categories: machine learning
---

<h3> 原文链接 </h3>

[Bo Moore](http://www.galvanize.com/blog/how-doc2vec-helps-data-scientists-defeat-data-sparsity/#.Vty1XYwrJL8)

本文包含 Mike Tamir 的贡献。他是 Galvanize 的首席科学家和学习官。

<hr>
<p></p>

<p>
我们曾经讨论过 <a href="http://www.galvanize.com/blog/add-and-subtract-words-like-vectors-with-word2vec-2">Word2Vec</a> 的强大功能。这是一种生成单词表征的方法，生成的单词表征可以像数学矢量一样运算。但它究竟为我们做了什么？为什么它那么重要？
</p>

<p>
数据总是越多越好，是吧？当为一系列文档构造数据矩阵时，是不是应该把任何能得到的信息都包含进去呢？答案其实是，不一定。
</p>

<p>
比如说你想为一系列专业期刊文章分类，而你的目标是把所有医学相关文章与别的分开。为了在数据矩阵中表达这些文档，你要做的是为每个出现过的单词保存出现的次数。对每个文档 (对应矩阵的一行)，你列出每个词库中的单词出现的次数。
</p>

<p>
问题是，这样做数据维度太高。并且，即便是一本很厚的书，它包含的单词也仍然只是词库的一小部分。这意味着你需要几十万甚至上百万维的向量来记录单词频度——而这个向量的大部分分量都是零。
</p>

<p>
这就是“稀疏性”——特征数很多但每个特征的信息量很小。稀疏性是有用信号的敌人，所以需要找到更强壮的、更紧凑的数据表征方式，提取最有用的特征。
</p>

<p>
回到医学文档分类的例子。我们想要做的是，从数据空间中划出一道边界，一边包含且只包含医学文献，另一边是跟医学无关的。
</p>

<p>
稀疏数据的问题是，你的算法也许能对已有的数据点正确划分，但对以后新加的就未必管用了。这就是数据密度很重要的原因。你希望数据量够大，这样新加的数据不会大幅度改变结果。
</p>

<p>
如何降低维度？一种方法是，找出最特别的那些词汇。对于医学文献，遍历所有文章找出出现或不出现的词汇。比如，可能医学文献会出现医生、护士、解剖这样的词汇，而在非医学文献中较少出现。这相当于 regularization。
</p>

<p>
另一种方法是排除掉出现频度很高的普通单词，比如 the 这样的 stop words。特别稀罕的单词同样不提供多少信息，可以去掉，就算它们跟文档内容很相关。
</p>

<p>
虽然这些方法是个不错的开始，它们的效果通常并不十分显著，不能把上百万维的向量约简为几百维。这就是 Word2Vec 上场的时候。
</p>

<p>
Word2Vec 把词库里的每个词变成一个唯一的矢量。这些矢量之间可以加减，就像真正的矢量一样。当把 Word2Vec 应用到整个文档后，词汇的矢量可以通过不同数学方法组合，得到一个集聚矢量，代表这个文档整体的话题方向。这个过程称为 Doc2Vec。
</p>

<p>
Doc2Vec 让你把巨大维数空间中的文档表征转换为低维数据空间。这个过程里的诀窍是要找出要在多大程度上压缩矩阵维度。如果压缩太多，不同类数据之间距离会太小或者会重叠。你希望的是在两者之间达成平衡。当数据维度太高时，很容易对数据做任意的划分而没有错误，但这种划分之所以可能，仅仅是因为维度高而已，没有体现数据本身由于类别不同自然形成的分离；当维度太低，则即便有自然分离，也会被压缩到一起。如何找到这种平衡呢？只能靠手工调节吗？
</p>
