---
layout: post
title:  "世仇与旧友：小说中动态关系的无监督学习"
date:   2016-04-12 Tue 02:18:20
categories: machine learning
---

这是一篇翻译文章。原文作者是 [Mohit Iyyer](https://www.cs.umd.edu/~miyyer/) 等人。

<h3>
原文链接
</h3>
[Feuding Families and Former Friends: Unsupervised Learning for Dynamic Fictional Relationships](https://www.cs.umd.edu/~miyyer/pubs/2016_naacl_relationships.pdf)

<hr>
<p></p>

<section>
<h2>摘要</h2>
<p>
理解小说两个角色之间的关系如何随着时间变化 (比如从最好的朋友变成死敌) 是数字人文里的一个主要挑战。我们为这个任务提出一种新的非监督神经网络，引入词典学习来生成可解释的精确关系轨迹。之前描述文学关系的工作依赖于用预先规定好的标签做了标注的剧情概要，而我们的模型从小说原文数据集学到一系列全局关系描述符和每种关系的描述符的轨迹。我们发现我们的模型学到了事件的描述符 (比如婚礼或谋杀)，以及人际状态描述符 (比如爱与悲伤)。我们的模型在两个众包任务上超过了话题模型的基准线，并且我们发现了与一个先存数据集中的标注的关系。
</p>
</section>

<section>
<h2>1. 描述角色关系</h2>
<p>
当书中两个角色分面包时，他们的这一餐这是处于生理需求，还是意味着更多？ Cognard-Black et al. (2014) 认为这种简单的互动反映了角色的多样性和背景，而 Foster (2009) 认为一顿饭的气氛可以预示一本书余下部分的走向。为了支持这些理论，学者们用他们的文学专长在不同书之间建立联系：Gabriel Conroy’s
dissonance from his family at a sumptuous feast in
Joyce’s The Dead, the frustration of Tyler’s mother in
Dinner at the Homesick Restaurant, and the grudging respect for a blind man eating meatloaf in Carver’s
Cathedral.
</p>

<p>
然而，这些洞察来之不易。经年累月的细致阅读和内化才能在书籍之间建立联系，这意味着关系的对称性和原型很可能继续隐藏在每年出版的成千上万本书中，除非文学专家主动地去搜集这些关系。
</p>

<p>
自然语言处理技术可以发现文本中的模式 (Jockers, 2013)，被越来越多地用来辅助这些文学探索。第 6 节我们回顾已有的利用固定标签分类或聚类书中角色关系 (朋友或敌人) 的技术。但这些技术忽略了不在已有语汇中的角色互动，并且不能处理关系在一本书的进程中演化动态性，比如 the vampiric downfall of Lucy and Arthur’s engagement in Dracula (Figure 1) or Winston Smith’s rat-induced betrayal of Julia in 1984.
</p>

<img src="{{ site.url }}/pictures/2016-04-12-fictional-relationship-Fig-1.png" id="Fig1">
<p class="image-caption">图 1: 描述关系动态演化轨迹的示例。目标是在无监督的前提下从虚构文学作品的原始文本学到描述符及其演化轨迹。</p></img>

<p>
为了处理这个问题，我们提出非监督关系建模的任务，让模型为每对文学形象同时学习关系描述符集合以及关系演化轨迹。不同于赋予一个特定关系单个描述符，学到的轨迹是一个描述符序列 (图 1)。
</p>

<p>
Gruber et al. (2007) 的贝叶斯隐含话题马尔科夫模型 (HTMM) 自然称为我们这个任务的选择，因为它可以计算关系描述符 (以话题的形式)，且有额外的时间组件。但是我们的实验表明 HTMM 学到的描述符会不自洽，且等多关注于事件和环境 (比如饭局，室外) 而不是人际状态如喜悦和悲伤等。
</p>

<p>
受近期深度学习进展的鼓舞，我们提出关系建模网络 (RMN)，一个深度复现自动编码器的新变种，引入词典学习来学习关系描述符。在两个众包评价上 RMN 取得了比 HTMM 和别的话题模型基准线更好的描述符一致性和轨迹精度 (第 4 节)。第 5 节讲了定性结果并与已有的文学研究建立了联系。
</p>
</section>

<section>
<h2>2. 一个角色互动数据集</h2>
<p>
我们的数据集包含来自 Project Gutenberg 和其它网络来源的 1383 部虚构作品。前者包含有限的主要是古典的文学作品 (不含科幻)，所以我们加了更多的不同风格的当代小说 (推理，言情，奇幻)。
</p>

<p>
为了识别角色提及，我们运行了 Book-NLP 流程 (Bamman et al. 2014)；这流程包含角色名字聚类，被引用角色的识别，以及共同指代的分辨。对每个检测到的角色提及，我们定义一个跨越提及之前 100 个和之后 100 个词的区间不同作者变化很大 (e.g., William Faulkner routinely wrote single sentences longer than many of Hemingway’s paragraphs)。我们数据集里的所有区间都包含刚好对两个角色的提及。这是个相当严格的要求，强制减小了数据大小，而包含多于两个角色的区间一般噪音较多。
</p>

<p>
识别出可用的区间后，我们过滤掉那些少于五个区间的关系。不这样做的话，我们的数据集会被不重要角色的短暂关系占据；这是不想要的，因为我们关注的是长期的、可变的关系。最后，我们过滤了词库，删掉了 500 个最常见的词语，以及那些在少于 100 本书中出现的词语。后一个步骤帮助纠正时间阶段和流派导致的变化 (e.g., “thou” and “thy” found in older works
like the Canterbury Tales)。最终的数据集包含 20013 个关系和 380408 个区间，而词库包含 16223 个词。
</p>
</section>

<section>
<h2>3. 关系建模网络</h2>
<p>
本节给出将 RMN 用于关系建模的数学描述。我们的模型在思想上跟话题模型类似：给定一个输入数据集，RMN 的输出是关系描述符 (话题) 集合，以及——对数据集中的每个关系——一个轨迹，或者一系列这些描述符的概率分布 (文档-话题分派)。然而，RMN 使用了深度学习来更好地控制描述符一致性和轨迹平滑性 (第 4 节)。
</p>

<h3>3.1 问题的形式化</h3>
<p>
假定 \(b\) 这本书里有两个角色 \(c_1\) 和 \(c_2\)。定义 \(S_{c_1,c_2}\) 为由词语区间组成的序列，其中每个区间 \(S_t \in S_{c_1,c_2}\) 本身也是词语组成的集合 \(\{w_1, w_2, \ldots, w_l\}\)，包含对 \(c_1\) 和 \(c_2\) 的同时提及；这里 \(l\) 是固定的。换句话说，\(S_{c_1,c_2}\) 包含每个 \(c_1\) 和 \(c_2\) 同时出现的场景对应的文本，且按时间顺序排列。
</p>

<h3>3.2 模型描述</h3>
<p>
与其它用于自然语言处理的神经网络模型一样，一开始我们为每类单词 \(w\) 赋予一个实向量嵌入 \(v_w \in \mathbb{R}^d\)。这些嵌入是 \(V\times d\) 矩阵 \(\mathbf{L}\) 的行，\(V\) 是词库大小。类似地，角色和书有自己的嵌入矩阵 \(\mathbf{C}\) 和 \(\mathbf{B}\)。我们希望 \(\mathbf{B}\) 抓住全局上下文信息 (比如 “Moby Dick” takes place at sea)，而 \(\mathbf{C}\) 抓住角色的那些不因所处关系而变的方面  (比如 Javert is a police officer)。最后，RMN 还学习到关系描述符的嵌入，这需要大小为 \(K\times d\) 的矩阵 \(\mathbf{R}\)，这里 \(K\) 是描述符的个数，类似于话题模型中话题的个数。
</p>

<p>
每个 RMN 的输入是一个元组，包含一本书和两个角色的编码，以及对应他们关系的区间：\((b, c_1, c_2, S_{c_1,c_2})\)。对每个这样的输入，目标是从通过 \(\mathbf{R}\) 里的关系描述符的线性组合重构 \(S_{c_1,c_2}\)；参见图 2。我们现在正式描述这个过程。
</p>

<img src="{{ site.url }}/pictures/2016-04-12-fictional-relationship-Fig-2.png">
<p class="image-caption">图 2：RMN 单次计算的例子。模型用 \(\mathbf{R}\) 里描述符的线性组合来近似一个输入区间 \(v_{s_t}\) 的矢量覆盖。描述符权重 \(d_t\) 定义了每个时间点的关系状态——当视为一个序列时——构成一个关系轨迹。</p></img>

<h4>3.2.1 用矢量平均模拟区间</h4>
<p>
\(S_{c_1,c_2}\) 里的每个区间 \(s_t\) 的矢量表达通过其中所有词的矢量的平均来计算
\[
    v_{s_t} = \frac{1}{l}\sum_{w\in s_t} v_w.
\]
然后我们把 \(v_{s_t}\) 与角色嵌入 \(v_{c_1}\) 和 \(v_{c_2}\) 以及书嵌入 \(v_b\) 拼接到一起形成一个大矢量，输入给标准的前馈层并得到隐含态 \(h_t\)，
\[
    h_t = f(W_h \cdot \left[v_{s_t}; v_{c_1}; v_{c_2}; v_b\right]).
\]
在所有实验中，变换矩阵 \(\mathbf{W}_h\) 都是 \(d\times4d\) 维。我们把 \(f\) 取为 \(\mathrm{ReLu}\) 函数：\(\mathrm{ReLu}(x) = \mathrm{max}(0,x)\)。
</p>

<h4>3.2.2 用关系描述符来近似区间</h4>
<p>
有了区间的表示，我们就来通过词典学习 (Olshausen and Field, 1997; Elad and Aharon, 2006) 的一个变型来学习描述符，这里描述符矩阵 \(\mathbf{R}\) 是词典，而我们试图用词典里条目的线性组合来近似输入的区间。
</p>

<p>
假定我们对 \(S_{c_1,c_2}\) 里的每个区间 \(s_t\) 计算隐含态。给定一个 \(h_t\)，我们利用某种复合函数 \(g\) 计算一个针对 \(K\) 个关系描述符上的权重矢量 \(d_t\)；\(g\) 在下节讲。概念上，每个 \(d_t\) 是个关系状态，而一个关系轨迹是一系列时间排序的关系状态 (<a href="#Fig1"> 图 1</a>)。计算了 \(d_t\) 后，我们用下面的加权平均来计算重构矢量 \(r_t\):
\[
    r_t = \mathbf{R}^\mathsf{T} d_t.
\]
我们的目标是让 \(r_t\) 与 \(v_{s_t}\) 相似。我们使用一个对比性的最大边界目标函数 (Weston et al., 2011; Socher et al., 2014)。我们从数据集中随机取出一些区间并计算每个取出的区间的矢量平均。这个区间矢量组成的子集记为 \(N\)。未正规化的目标函数 \(J\) 是个 Hinge loss 函数，它最小化 \(r_t\) 和阴性样本的内积，同时最大化 \(r_t\) 和 \(v_{s_t}\) 的内积，
\[
    J(\theta) = \sum_{t=0}^{|S_{c_1,c_2}|} \sum_{n\in N} \max(0, 1 - r_t v_t + r_t v_{s_n}),
\]
这里 \(\theta\) 是模型参数。
</p>

<h4>3.2.3 计算描述符上的权重</h4>
<p>
应该选取何种复合函数 \(g\) 来表达给定时间点的关系状态？表面上看，这显得简单；我们可以把 \(h_t\) 投影到 \(K\) 维，然后使用 \(\mathrm{softmax}\) 或别的非线性函数来得到非负权重 (我们试了不同的非线性函数，发现 \(\mathrm{softmax}\) 给出的结果最有可解释性，因为它倾向于选出单个的描述符)。但是，这个方法忽略了前一个时间点的关系状态。为了模拟关系对时间的依赖，我们可以添加一个重现连接，
\[
    d_t = \mathrm{softmax}(W_d \cdot \left[h_t; d_{t-1}\right]),
\]
这里 \(W_d\) 的大小是 \(K\times(d+K)\)，而
\[
    \mathrm{softmax}(q) = \frac{e^{q}}{\sum_{j=1}^k e^{q_j}}。
\]
</p>

<p>
我们希望这个重现连接可以把过去关系中的一部分携带到当前时间点。在时刻 \(t\) 相爱的两个人不大可能在时刻 \(t+1\) 分手，就算 \(s_{t+1}\) 不包含任何跟爱有关的词。但是，由于目标函数最大化的是与当前时刻的输入的相似性，模型并没有被强迫从前一个状态到当前状态是平滑过渡。一个补救措施是让模型预言下一个时刻的输入，但发现这样很难优化。
</p>

<p>
我们于是通过修改上面的方程来强迫模型使用上次的关系状态
\[
    d_t = \alpha \cdot \mathrm{softmax}(W_d \cdot \left[h_t; d_{t-1}\right]) + (1-\alpha) \cdot d_{t-1},
\]
这里 \(\alpha\) 是 0 到 1 之间的数。我们试过把 \(\alpha\) 固定为 0.5，以及让模型通过下面的关系来学习 \(\alpha\)
\[
    \alpha = \sigma\left(v_\alpha^\mathsf{T} \cdot \left[h_t;d_{t-1};v_{s_t}\right]\right),
\]
这里 \(\sigma\) 是 sigmoid 函数，而 \(v_\alpha\) 是一个 \(2d + K\) 维的矢量。最开始固定 \(\alpha=0.5\) 然后在别的参数收敛后在调节 \(\alpha\) 可以提到训练的稳定性；关于超参数参见第 4 节 (参考了词典学习中的 alternatice minimization 策略，其中词典和权重分开学习。参考 Agarwal et al., 2014)。
</p>

<h4>3.2.4 解释描述符并强化唯一性</h4>
<p>
每个描述符是 \(\mathbf{R}\) 的 \(d\)-维行矢量。由于我们的目标函数 \(J\) 强制这些描述符跟单词嵌入 \(\mathbf{L}\) 处于同一个矢量空间，我们可以用余弦距离查看 \(\mathbf{L}\) 中的近邻矢量来做出解释。
</p>

<p>
为了避免描述符之间过于相似，我们又添加了一个惩罚项 \(X\):
\[
    X(\theta) = \left\lVert\mathbf{R}\mathbf{R}^\mathsf{T} - \mathbf{I}\right\rVert,
\]
这里 \(\mathbf{I}\) 是单位矩阵。这个项来自独立成分分析中分量的正交性约束 (Hyvarinen and Oja 2000)。
</p>

<p>
我们把 \(J\) 和 \(X\) 加到一起得到最终的训练目标
\[
    L(\theta) = J(\theta) + \lambda X(\theta),
\]
这里 \(\lambda\) 是控制唯一性惩罚的强度的超参数。
</p>
</section>

<section>
<h2>4. 评估描述符和轨迹</h2>
<p>
由于以前还没有工作探究非监督关系模型随时间的可解释性，对 RMN 不太好评估。更大的问题是这个任务本来就有主观性，比如，一个漏掉了重大事件的轨迹是否比幻想出原文不存在的情节的轨迹更好？
</p>

<p>
注意到这些问题，我们进行了三轮评估来说明我们的输出是合理的。首先，我们进行了众包可解释性实验，结果表明 RMN 给出的描述符比三个话题模型基准线的更一致。第二个众包任务表面我们的模型产生的轨迹比话题模型更符合剧情概要。最后，我们定性比较了 RMN 的输入和已有的对文学关系的静态标注，发现了预期的和令人吃惊的结果。
</p>

<h3>4.1 话题模型基准线</h3>
<p>
在进入评估之前，我们简单描述一下三个基准线方法，它们都是贝叶斯生成模型。隐含狄利克雷分派 (LDA; Blei et al. 2003) 为每个文档学到单个文档-话题分布；我们可以通过把一个关系的所有区间拼接成一个文档来把 LDA 应用到我们的数据集。类似地，NUBBI (Chang et al. 2009a) 为关系和单个角色分别学到话题集。
</p>

<h3>4.2 实验设置</h3>
<h3>4.3 描述符有意义吗</h3>
<h3>4.4 演化轨迹有意义吗</h3>
<h3>4.5 什么让关系称为正面关系</h3>
</section>

<section>
<h2>5. 定型分析</h2>
</section>

<section>
<h2>6. 相关的工作</h2>
</section>

<section>
<h2>7. 结论</h2>
</section>

<section>
<h2>致谢</h2>
</section>
